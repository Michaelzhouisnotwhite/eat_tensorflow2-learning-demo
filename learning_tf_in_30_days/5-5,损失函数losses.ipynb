{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd8f8ae",
   "metadata": {},
   "source": [
    "# 5-5,损失函数losses\n",
    "\n",
    "一般来说，监督学习的目标函数由损失函数和正则化项组成。（Objective = Loss + Regularization）\n",
    "\n",
    "对于keras模型，目标函数中的正则化项一般在各层中指定，例如使用Dense的 kernel_regularizer 和 bias_regularizer等参数指定权重使用l1或者l2正则化项，此外还可以用kernel_constraint 和 bias_constraint等参数约束权重的取值范围，这也是一种正则化手段。\n",
    "\n",
    "损失函数在模型编译时候指定。对于回归模型，通常使用的损失函数是均方损失函数 mean_squared_error。\n",
    "\n",
    "对于二分类模型，通常使用的是二元交叉熵损失函数 binary_crossentropy。\n",
    "\n",
    "对于多分类模型，如果label是one-hot编码的，则使用类别交叉熵损失函数 categorical_crossentropy。如果label是类别序号编码的，则需要使用稀疏类别交叉熵损失函数 sparse_categorical_crossentropy。\n",
    "\n",
    "如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c6cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,regularizers,constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa2829",
   "metadata": {},
   "source": [
    "### 一，损失函数和正则化项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f800b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01), \n",
    "                activity_regularizer=regularizers.l1(0.01),\n",
    "                kernel_constraint = constraints.MaxNorm(max_value=2, axis=0))) \n",
    "model.add(layers.Dense(10,\n",
    "        kernel_regularizer=regularizers.l1_l2(0.01,0.01),activation = \"sigmoid\"))\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "        loss = \"binary_crossentropy\",metrics = [\"AUC\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a10b65",
   "metadata": {},
   "source": [
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 64)                4160      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                650       \n",
    "=================================================================\n",
    "Total params: 4,810\n",
    "Trainable params: 4,810\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b70934",
   "metadata": {},
   "source": [
    "### 二，内置损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed080226",
   "metadata": {},
   "source": [
    "内置的损失函数一般有类的实现和函数的实现两种形式。\n",
    "\n",
    "如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。\n",
    "\n",
    "常用的一些内置损失函数说明如下。\n",
    "\n",
    "* mean_squared_error（均方误差损失，用于回归，简写为 mse, 类与函数实现形式分别为 MeanSquaredError 和 MSE）\n",
    "\n",
    "* mean_absolute_error (平均绝对值误差损失，用于回归，简写为 mae, 类与函数实现形式分别为 MeanAbsoluteError 和 MAE)\n",
    "\n",
    "* mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类与函数实现形式分别为 MeanAbsolutePercentageError 和 MAPE)\n",
    "\n",
    "* Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)\n",
    "\n",
    "* binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)\n",
    "\n",
    "* categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)\n",
    "\n",
    "* sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)\n",
    "\n",
    "* hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)\n",
    "\n",
    "* kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类与函数实现形式分别为 KLDivergence 或 KLD)\n",
    "\n",
    "* cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485e6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2bc818d",
   "metadata": {},
   "source": [
    "### 三，自定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0438d",
   "metadata": {},
   "source": [
    "自定义损失函数接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。\n",
    "\n",
    "也可以对tf.keras.losses.Loss进行子类化，重写call方法实现损失的计算逻辑，从而得到损失函数的类的实现。\n",
    "\n",
    "下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。\n",
    "\n",
    "它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。\n",
    "\n",
    "它有两个可调参数，alpha参数和gamma参数。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。\n",
    "\n",
    "从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal Loss。\n",
    "\n",
    "详见《5分钟理解Focal Loss与GHM——解决样本不平衡利器》\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/80594704"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f831e0",
   "metadata": {},
   "source": [
    "$$focal\\_loss(y,p) = \\begin{cases}\n",
    "-\\alpha  (1-p)^{\\gamma}\\log(p) &\n",
    "\\text{if y = 1}\\\\\n",
    "-(1-\\alpha) p^{\\gamma}\\log(1-p) &\n",
    "\\text{if y = 0}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=0.75):\n",
    "    \n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        bce = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = -1 )\n",
    "        return loss\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self,gamma=2.0,alpha=0.75,name = \"focal_loss\"):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self,y_true,y_pred):\n",
    "        bce = tf.losses.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
    "        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, self.gamma)\n",
    "        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = -1 )\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56ae06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2445fc76",
   "metadata": {},
   "source": [
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"算法美食屋\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "也可以在公众号后台回复关键字：**加群**，加入读者交流群和大家讨论。\n",
    "\n",
    "![算法美食屋二维码.jpg](./data/算法美食屋二维码.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e014b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
